<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Welcome to my blog!</title>
  
  
  <link href="https://overrideturing.github.io/atom.xml" rel="self"/>
  
  <link href="https://overrideturing.github.io/"/>
  <updated>2024-11-04T06:55:00.393Z</updated>
  <id>https://overrideturing.github.io/</id>
  
  <author>
    <name>OverrideTuring</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>泛函分析重要问题记录</title>
    <link href="https://overrideturing.github.io/2024/11/04/%E6%B3%9B%E5%87%BD%E5%88%86%E6%9E%90%E9%87%8D%E8%A6%81%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/"/>
    <id>https://overrideturing.github.io/2024/11/04/%E6%B3%9B%E5%87%BD%E5%88%86%E6%9E%90%E9%87%8D%E8%A6%81%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/</id>
    <published>2024-11-04T06:32:08.000Z</published>
    <updated>2024-11-04T06:55:00.393Z</updated>
    
    <content type="html"><![CDATA[<h3 id="距离空间"><a href="#距离空间" class="headerlink" title="距离空间"></a>距离空间</h3><ol><li><p><strong>证明</strong>：Arzela定理：$C[a,b]$中的子集$A$是列紧的，当且仅当$A$中的函数是<strong>一致有界和等度连续</strong>的.</p><p>即存在$K&gt;0$，使得对于每一点$t\in [a,b]$及一切$x\in A$，有$\vert x(t)\vert \leq K$，并且对于任意的$\varepsilon &gt; 0$，存在$\delta &gt; 0$，当$\vert t_1 - t_2 \vert &lt; \delta$时，$\vert x(t_1)-x(t_2)\vert &lt; \varepsilon (\forall x\in A)$.</p></li><li><p><strong>证明</strong>：距离空间$(X,d)$上的压缩映射原理（也叫Banach不动点定理）。</p></li><li><p><strong>证明</strong>：任何距离空间都可以完备化。（同理可证：任何赋范线性空间都可以完备化）</p></li></ol><h3 id="赋范空间"><a href="#赋范空间" class="headerlink" title="赋范空间"></a>赋范空间</h3><ol><li><p><strong>证明</strong>：$L^p(E)(1\leq p &lt; \infty)$是Banach空间（即完备的赋范空间）。</p></li><li><p><strong>证明</strong>：$L^p[a,b]$是可分的（具有可数稠密子集）。</p></li><li><p><strong>证明</strong>：$L^\infty(E)$是不可分的Banach空间。</p></li><li><p><strong>证明</strong>：对于任意的$x(t)\in L^\infty(E),m(E)&lt;\infty$，有$\Vert x\Vert_p \rightarrow \Vert x\Vert_\infty$，即</p><script type="math/tex; mode=display">\lim_{p \rightarrow +\infty}\left(\int_E\vert x(t)\vert^p dt\right)^\frac{1}{p}=\Vert x\Vert_\infty</script><p><strong>证明如下</strong>：</p><script type="math/tex; mode=display">由\Vert x\Vert_\infty定义可知，\forall \varepsilon >0，存在集合E_1 \subset E，m(E_1)>0，\\s.t. \forall t\in E_1，\Vert x\Vert_\infty - \varepsilon \leq x(t)\leq \Vert x\Vert_\infty \\则\left(\int_{E_1}(\Vert x\Vert_\infty - \varepsilon)^p dt\right)^\frac{1}{p} \leq\left(\int_E\vert x(t)\vert^p dt\right)^\frac{1}{p} \leq\left(\int_E\Vert x\Vert_\infty^p dt\right)^\frac{1}{p}\\即(\Vert x\Vert_\infty - \varepsilon)m(E_1)^\frac{1}{p} \leq\left(\int_E\vert x(t)\vert^p dt\right)^\frac{1}{p} \leq\Vert x\Vert_\infty m(E)^\frac{1}{p}\\对不等式取极限有\Vert x\Vert_\infty - \varepsilon \leq\lim_{p \rightarrow +\infty}\left(\int_E\vert x(t)\vert^p dt\right)^\frac{1}{p} \leq\Vert x\Vert_\infty\\由\varepsilon 的任意性可得\lim_{p \rightarrow +\infty}\left(\int_E\vert x(t)\vert^p dt\right)^\frac{1}{p}=\Vert x\Vert_\infty</script></li><li><p><strong>证明</strong>：赋范线性空间$X$的一个子空间$X_1$是开集，则$X_1=X$。</p></li><li><p><strong>证明</strong>：Riese引理：设$(X,\Vert \cdot \Vert)$是一个赋范空间，$X_0$是$X$的真闭子空间，则对于$\forall\varepsilon &gt;0$，存在$x_0\in X$，使得$\Vert x_0 \Vert=1$，且对于$\forall x\in X_0$，</p><script type="math/tex; mode=display">\Vert x-x_0\Vert > 1-\varepsilon.</script></li><li><p><strong>证明</strong>：任意实的$n$维赋范空间必与$R^n$代数同构，拓扑同胚。</p></li><li><p><strong>证明</strong>：赋范空间是有限维的当且仅当$X$中的任何有界集是列紧的。</p></li><li><p><strong>证明</strong>：设$X$是Banach空间，$M$是$X$的闭子空间，则赋范空间$X$关于$M$的商空间$X/M$是Banach空间。</p></li></ol><h3 id="内积空间"><a href="#内积空间" class="headerlink" title="内积空间"></a>内积空间</h3><ol><li><p><strong>证明</strong>：Schwarz不等式：设$H$是内积空间，对于$\forall x,y\in H$，有</p><script type="math/tex; mode=display">\Vert (x,y)\Vert^2 \leq (x,x)(y,y).</script></li><li><p><strong>证明</strong>：正交列完备的条件是它是最大的正交列，此时Parseval等式成立（四个等价命题）。</p></li></ol><h3 id="线性算子和线性泛函"><a href="#线性算子和线性泛函" class="headerlink" title="线性算子和线性泛函"></a>线性算子和线性泛函</h3><ol><li><p><strong>证明</strong>：线性算子连续则必有界，有界则必连续。</p></li><li><p><strong>证明</strong>：设$(X,\Vert \cdot \Vert)$是有限维赋范空间，$(Y,\Vert \cdot \Vert)$是任意一个赋范空间，$T$是从$X$到$Y$的线性算子，则$T$是有界线性算子。</p></li><li><p><strong>证明</strong>：Baire纲定理：完备的距离空间是第二纲集。（即不能由至多可数个疏集的并表示）</p></li><li><p><strong>证明</strong>：一致有界原则：设$\{T_\alpha | \alpha \in I \}$是Banach空间$X$上到赋范空间$X_1$中的有界线性算子族，如果对于$\forall x \in X$，有</p><script type="math/tex; mode=display">\sup_\alpha \Vert T_\alpha x\Vert < \infty,</script><p> 则$\{T_\alpha | \alpha \in I \}$是有界集。</p></li><li><p><strong>证明</strong>：开映射定理：Banach空间的有界线性算子将开集映射成开集。</p></li><li><p><strong>证明</strong>：逆算子定理：设$T$是从Banach空间$X$上到Banach空间$X_1$上的一对一的有界线性算子，则$T$的逆算子存在，且$T^{-1}$是有界的。</p></li><li><p><strong>证明</strong>：闭图像定理：设$T$是Banach空间$X$上到Banach空间$X_1$中的闭线性算子，则$T$是有界线性算子。</p></li><li><p><strong>证明</strong>：复的Hahn-Banach定理：设$X$是一个复的赋范空间，$G$是$X$的子空间，$f$是$G$上的有界线性泛函，则$f$可以保持范数不变地延拓到全空间$X$上。</p><p> 即存在$X$上的有界线性泛函$F$，使得</p><p> (1) 对于$\forall x\in G,F(x)=f(x)$；</p><p> (2) $\Vert F \Vert = \Vert f \Vert_G$，</p><p> 其中$\Vert f \Vert_G$表示$f$作为$G$上的有界线性泛函的范数。</p><p> 提示：证明过程会用到Zorn引理，设$P$是非空半序集，如果$P$中任何全序子集都有上界，则$P$至少有一个极大元。</p></li></ol><h3 id="共轭空间和共轭算子"><a href="#共轭空间和共轭算子" class="headerlink" title="共轭空间和共轭算子"></a>共轭空间和共轭算子</h3><ol><li><p><strong>证明</strong>：$f$是$L^p[a,b]$上的有界线性泛函，则存在唯一的$y(t)\in L^q[a,b]$，$(\frac{1}{p}+\frac{1}{q}=1)$，使得</p><script type="math/tex; mode=display">f(x)=\int^b_a x(t)y(t)dt, \forall x \in L^p[a,b],</script><p> 且</p><script type="math/tex; mode=display">\Vert f \Vert = \Vert y \Vert_q = (\int_a^b \vert y(t) \vert^q dt)^\frac{1}{q}.</script><p>反之，对于$\forall y \in L^q[a,b]$，上式定义了$L^p[a,b]$上的一个有界线性泛函。（注意：此定理证明要用到大量实变函数定理）</p></li><li><p><strong>证明</strong>：Riesz表示定理：设$H$是一个Hilbert空间，$f$是$H$上的有界线性泛函，则存在唯一的$y_f \in H$，使得</p><script type="math/tex; mode=display">f(x)=(x,y_f), \forall x \in H,</script><p>并且</p><script type="math/tex; mode=display">\Vert f \Vert = \Vert y_f \Vert.</script><p>（提示：在该定理证明过程中，应该深刻理解维度的概念，感受泛函分析的美妙）</p></li><li><p><strong>证明</strong>：$L^p(1&lt;p&lt;\infty)$是自反的，$l^p(1&lt;p&lt;\infty)$是自反的。</p></li><li><p><strong>证明</strong>：如果$\{x_n\}<br> $弱收敛，则$\{\Vert x_n \Vert\}$有界。（提示：用一致有界原则，体会数学思想）</p></li><li><p><strong>证明</strong>：设$X$是Banach空间，$T\in B(X)$，则$\sigma(T)$是有界集。</p></li><li><p><strong>证明</strong>：设$T$是Banach空间$X$到$X$的有界线性算子，$\lambda \in \rho(T)$，且$\vert \mu \vert &lt; \Vert (\lambda I - T)^{-1}\Vert^{-1}$，则$\lambda + \mu \in \rho(T)$，即$\rho(T)$是一个开集。</p></li><li><p><strong>证明</strong>：设$T$是有界线性算子，则$\sigma(T)\neq \phi$。（注意：此定理难度超高，立足的空间来去变换，如若掌握证明技巧，对抽象思维大有裨益）</p></li><li><p><strong>证明</strong>：设$T\in B(X)$，则$r_\sigma(T)=\sup_{\lambda \in \sigma\{T\}} \vert \lambda \vert$。</p></li></ol><h3 id="谱理论"><a href="#谱理论" class="headerlink" title="谱理论"></a>谱理论</h3><ol><li><p><strong>证明</strong>：设$T$是Hilbert空间$H$上的自共轭算子，则$T$的剩余谱是空集。（提示：用到共轭算子零空间、正交、值域所具有的一些性质）</p></li><li><p><strong>证明</strong>：设$H$是Hilbert空间，线性算子$T\in B(H)$是紧的，当且仅当存在$B(H)$中的一列有穷秩算子$T_n$，使得$\Vert T-T_n\Vert \rightarrow 0(n\rightarrow\infty)$。</p></li><li><p><strong>证明</strong>：设$X$是Banach空间，$T_1,T_2\in B(X)$。则：</p><p> (1) $T_1$和$T_2$中有一个是紧的，那么$T_1T_2$是紧的。</p><p> (2) $T\in B(X)$，$T$是紧的当且仅当$T^*$是紧的。</p></li><li><p><strong>证明</strong>：恒等算子$I:X\rightarrow X$是紧的，当且仅当$\dim{X} &lt; \infty$。（提示：用Riesz引理）</p></li><li><p><strong>证明</strong>：设$T$是赋范空间$X$到$X$的紧线性算子，那么对于$\forall a&gt;0$，$T$的特征值$\lambda$满足$\vert \lambda \vert &gt; \alpha$的个数是有限的。</p></li><li><p><strong>证明</strong>：令$T$是从赋范空间$X$到$X$的紧线性算子，若$\lambda \neq 0$，则$\lambda I-T$的零空间$N(\lambda I-T)$是有限维的。</p></li><li><p><strong>证明</strong>：设$T$是从Banach空间$X$到$X$的紧线性算子，$\lambda \neq 0$，则$R(\lambda I-T)$是闭的。</p></li><li><p><strong>证明</strong>：$X$是Banach空间，$T\in K(X)$，$\lambda \neq 0$，若$N(\lambda I-T)=\{0\}$，则$R(\lambda I-T)=X$。</p></li><li><p><strong>证明</strong>：设$T$是从Hilbert空间$H$到$H$的紧线性算子，$\lambda \neq 0$，则</p><script type="math/tex; mode=display">H=N(\bar{\lambda}-T^*)\oplus R(\lambda I-T).</script></li><li><p><strong>证明</strong>：Fredholm二择一定理：设$T$是从Hilbert空间$H$到$H$的紧线性算子，$\lambda \neq 0$，则：</p><p>(i) 对于任意的$y\in H$，非齐次方程</p><script type="math/tex; mode=display">(\lambda I-T)x=y \tag{1}</script><p>   有唯一解的充要条件是齐次方程</p><script type="math/tex; mode=display">(\lambda I-T)x=0 \tag{2}</script><p>   没有非零解。</p><p>(ii) 若方程(2)有非零解，</p><p>   非齐次方程(1)有解的充要条件是$y$与方程</p><script type="math/tex; mode=display">(\bar{\lambda}I-T^*)x=0 \tag{3}</script><p>   的所有解正交，即：$y\perp N(\bar{\lambda}I-T^*)$。</p></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;距离空间&quot;&gt;&lt;a href=&quot;#距离空间&quot; class=&quot;headerlink&quot; title=&quot;距离空间&quot;&gt;&lt;/a&gt;距离空间&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;证明&lt;/strong&gt;：Arzela定理：$C[a,b]$中的子集$A$是列紧的，当且仅当</summary>
      
    
    
    
    
    <category term="mathematic" scheme="https://overrideturing.github.io/tags/mathematic/"/>
    
  </entry>
  
  <entry>
    <title>实变函数重要问题记录</title>
    <link href="https://overrideturing.github.io/2024/11/04/%E5%AE%9E%E5%8F%98%E5%87%BD%E6%95%B0%E9%87%8D%E8%A6%81%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/"/>
    <id>https://overrideturing.github.io/2024/11/04/%E5%AE%9E%E5%8F%98%E5%87%BD%E6%95%B0%E9%87%8D%E8%A6%81%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/</id>
    <published>2024-11-04T06:32:00.000Z</published>
    <updated>2024-11-04T08:43:00.428Z</updated>
    
    <content type="html"><![CDATA[<ol><li><p><strong>证明</strong>：康托尔-伯恩斯坦(Cantor-Bernstein)定理：对于集合A和B，存在单射$f:A\rightarrow B$和$g:B\rightarrow A$，则存在一个双射$h:A\rightarrow B$。（这样两个集合就等势）</p></li><li><p><strong>证明</strong>：一个集合为无限集当且仅当它和自己的一个真子集可建立一一对应关系。</p></li><li><p><strong>证明</strong>：在区间[0,1]内，不存在这样的函数，在有理数点处连续，在无理数点处不连续。</p><p><strong>备注</strong>：反过来是成立的，即存在一种函数，在有理数点处不连续，在无理数点处连续，例如黎曼函数</p><script type="math/tex; mode=display">f(x)=\left\{\begin{matrix}p/q,&x=p/q，p,q都属于正整数，p/q为既约真分数 \\0,&x为无理数\end{matrix}\right.\\x\in [0,1]</script><p>实际上有一个更通用性的结论，定义在一个区间上的函数在某个稠密子集上连续，那么就找不到另一个函数，使得它们的不连续点互补。</p></li><li><p><strong>证明</strong>：Carathéodory定理说明了什么样的集合是可测集，它的等价定义是，一个集合E是可测集，那么对于任意$\varepsilon &gt; 0$，存在一个闭集$F\subset E$，使得$m^*(E-F)&lt;\varepsilon$。同样，还有两种闭集的等价定义，参看川大实变函数课程P35。</p></li><li><p><strong>证明</strong>：有两个Lebesgue可测函数$f$和$g$，$f+g$也是勒贝格可测函数。</p></li><li><p><strong>证明</strong>：可测函数三大定理：Egorov定理（几乎一致收敛）、Riesz定理（子列几乎收敛）、Lusin定理（依测度收敛于连续函数）</p><p><strong>重点概念</strong>：</p><ul><li><p>几乎处处收敛：指去掉一个零测集后，数列$\{f_k(x)\}$处处收敛于$f(x)$</p></li><li><p>依测度收敛：数列$\{f_k(x)\}$中不收敛于$f(x)$的点集在极限情况下是零测集</p></li></ul><p>（这两者并不等价，也不存在包含关系，函数序列可能处处不收敛，但依测度收敛，一种情况是，不收敛的点在变少，但在整个区间可能是浮动存在的）</p></li><li><p><strong>证明</strong>：有关可积函数的定理：Lebesgue控制收敛定理、Levi定理（勒贝格单调收敛定理）、Fatou引理、Fubini定理</p></li><li><p><strong>证明</strong>：Lebesgue定理：有界函数黎曼可积的充要条件是几乎处处连续。</p></li><li><p><strong>证明</strong>：积分全连续性：在E上可积，对于任意$\varepsilon&gt;0$，存在$\delta&gt;0$，如果$E’\subset E$，且$m(E’) &lt; \delta$，则$\int_E f(x)dx &lt; \varepsilon$。</p></li><li><p><strong>证明</strong>：全连续函数（不是积分全连续性）是有界变差函数，有界变差函数等价于两个单调递增函数的差。</p></li><li><p><strong>证明</strong>：Lebesgue积分意义下的牛顿-莱布尼茨公式（参看川大实变函数课程P43，网课未给出完整证明）</p></li><li><p><strong>证明</strong>：存在处处连续处处不可导的函数（构造式证明）</p></li><li><p><strong>证明</strong>：rising sun lemma（参考川大实变函数课程P44）</p><script type="math/tex; mode=display">设f:R\rightarrow R是连续函数，S=\left\{x \in R | \exists y>x, s.t. f(x)<f(y)\right\}，\\则集合S是开集，即S=\bigcup_k(a_k,b_k)，\\如果(a_k,b_k)是有限的，则f(b_k)-f(a_k)=0</script></li><li><p><strong>证明</strong>：$L^p$空间的三角不等式（参考川大实变函数课程P46，用到Holder不等式）</p><script type="math/tex; mode=display">\left[\int_a^b \vert f(x)+g(x)\vert ^p dx \right]^\frac{1}{p} \leq\left[\int_a^b \vert f(x)\vert ^p dx\right]^\frac{1}{p} +\left[\int_a^b \vert g(x)\vert ^p dx\right]^\frac{1}{p}</script><p><strong>证明如下</strong>：</p><script type="math/tex; mode=display">\int_a^b \vert f(x)+g(x)\vert ^p dx = \int_a^b \vert f(x)+g(x)\vert \vert f(x)+g(x)\vert^{p-1} dx\\\leq \int_a^b \vert f(x)\vert \vert f(x)+g(x)\vert^{p-1} dx + \int_a^b \vert f(x)+g(x)\vert \vert g(x)\vert^{p-1} dx\\\leq \left[\int_a^b \vert f(x)\vert^p dx\right]^\frac{1}{p} \left[\int_a^b \vert f(x)+g(x)\vert^p dx\right]^\frac{1}{q}+\\\left[\int_a^b \vert g(x)\vert^p dx\right]^\frac{1}{p} \left[\int_a^b \vert f(x)+g(x)\vert^p dx\right]^\frac{1}{q}(Holder不等式)\\=\left\{\left[\int_a^b \vert f(x)\vert^p dx\right]^\frac{1}{p}+\left[\int_a^b \vert g(x)\vert^q dx\right]^\frac{1}{q} \right\}\left[\int_a^b \vert f(x)+g(x)\vert^p dx\right]^\frac{1}{q}\\其中p和q满足\frac{1}{p}+\frac{1}{q}=1 (p>1,q>1)\\移项即得\left[\int_a^b \vert f(x)+g(x)\vert ^p dx \right]^\frac{1}{p} \leq\left[\int_a^b \vert f(x)\vert ^p dx\right]^\frac{1}{p} +\left[\int_a^b \vert g(x)\vert ^p dx\right]^\frac{1}{p}</script></li><li><p><strong>证明</strong>：Holder不等式</p><script type="math/tex; mode=display">如果\frac{1}{p}+\frac{1}{q}=1 (p,q>0)，则\Vert fg\Vert_1 \leq \Vert f\Vert_p \Vert g\Vert_q \\提示：已知Young不等式ab\leq \frac{a^p}{p}+\frac{b^q}{q} (a,b>0)\\</script><p><strong>证明如下</strong>：</p><script type="math/tex; mode=display">\int_a^b\frac{\vert f(x)g(x)\vert}{\Vert f\Vert_p \Vert g\Vert_q} dx\leq \int_a^b\frac{1}{p}\frac{\vert f(x)\vert^p}{\Vert f\Vert_p^p}+\frac{1}{q}\frac{\vert g(x)\vert^q}{\Vert f\Vert_g^g}dx=\frac{1}{p}+\frac{1}{q}=1</script></li></ol><p><br><br><strong>数学公式渲染太让人头疼，可以参考这个</strong>：</p><ul><li><a href="https://blog.csdn.net/Cool_breeze_/article/details/115104683?fromshare=blogdetail&amp;sharetype=blogdetail&amp;sharerId=115104683&amp;sharerefer=PC&amp;sharesource=qq_48947196&amp;sharefrom=from_link">如何在hexo中支持Mathjax</a></li><li>此外，把node_modules/hexo-renderer-mathjax/mathjax.html中的某script标签的src改为<code>https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML</code></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;证明&lt;/strong&gt;：康托尔-伯恩斯坦(Cantor-Bernstein)定理：对于集合A和B，存在单射$f:A&#92;rightarrow B$和$g:B&#92;rightarrow A$，则存在一个双射$h:A&#92;rightarrow B$。（这样</summary>
      
    
    
    
    
    <category term="mathematic" scheme="https://overrideturing.github.io/tags/mathematic/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion发展脉络</title>
    <link href="https://overrideturing.github.io/2024/11/04/diffusion%E5%8F%91%E5%B1%95%E8%84%89%E7%BB%9C/"/>
    <id>https://overrideturing.github.io/2024/11/04/diffusion%E5%8F%91%E5%B1%95%E8%84%89%E7%BB%9C/</id>
    <published>2024-11-03T17:36:48.000Z</published>
    <updated>2024-11-05T08:15:33.541Z</updated>
    
    <content type="html"><![CDATA[<p>2021年以来，生成式模型扛把子diffusion model红红火火，已成为当代人工智能研究者不得不面对的话题。本人作为初学者，最近学习了一些经典扩散模型如DDPM、DDIM、Classifier Guidance等，以及一些较新的模型如Stable Diffusion 3和Flow Matching等。趁着记忆犹新赶紧记录下来。本文梳理了diffusion model的发展脉络，并展望了未来diffusion model的发展方向。</p><h2 id="一、开山之作：DDPM"><a href="#一、开山之作：DDPM" class="headerlink" title="一、开山之作：DDPM"></a>一、开山之作：DDPM</h2><p>2020年以前，在生成模型(Generative Model)领域占统治地位的一直是GANs (Generative Adversarial Networks)。随后，一种去噪扩散概率模型DDPM (Denoising Diffusion Probabilistic Model)横空出世，凭借较小的参数量就能取得很好的图像生成效果，而且由于是概率模型，生成图像的多样性也能得到保证。因此，从2021年至今，扩散模型的研究就如雨后春笋般涌现。一条大致的发展路线如<strong>图1</strong>所示。从最开始随机生成图像的DDPM，到最后能根据自然语言prompt生成各种风格图像的Stable Diffusion，发展是显著且迅速的。在接下来的介绍中，会看到这样的成就并非一蹴而就，每一步工作都严格建立在前人的基础之上，最后逐渐累积到今天的成果。</p><pre class="mermaid">flowchart LRA[DDPM]B[DDIM]C[Classifier Guidance]D[Classifier-free Guidance]E[Latent Diffusion Model]F[Dalle2]G[Stable Diffusion]H[Flow Matching]I[Rectified Flow]A--加速采样过程-->B--分类器指导-->C--隐式条件指导-->D--隐空间推理-->EE-->FE-->GA--不同理解视角-->H--流的改进-->I</pre><center><p><b>图1</b> 扩散模型大致发展脉络</p></center><h3 id="1-扩散模型基本思想"><a href="#1-扩散模型基本思想" class="headerlink" title="1. 扩散模型基本思想"></a>1. 扩散模型基本思想</h3><p>首先，要从开山之作DDPM说起<font color="blue">[2]</font>，它把图像生成建模成去噪过程。假设有一张图像$x_0$，对它逐渐施加噪声得到一系列图像$x_1,x_2,…,x_T$。图像变得越来越模糊，直到施加$T$次噪声后，图像彻底变成标准正态分布$p(x_T)=\mathcal{N}(x_T;0,I)$。图像生成则是逆过程，从一张噪声图像开始，逐渐去噪得到原始图像。以上过程如<strong>图2</strong>所示。</p><p><img src="\images\blogs\diffusion\DDPM过程.png" alt="DDPM过程"></p><center><p><b>图2</b> DDPM过程示意（从左到右是加噪，从右到左是去噪）</p></center><p>加噪的过程可以表示为</p><script type="math/tex; mode=display">q(x_{1:T}|x_0)=\prod_{t=1}^Tq(x_t|x_{t-1})\tag{1}</script><p>这是一个随机过程。DDPM基于以下假设：</p><ul><li>它是马尔科夫随机过程，所以每个$x_t(t\neq 0)$仅由$x_{t-1}$决定；</li><li>每个中间图像都符合正态分布，它的均值取决于上一步的输出图像。</li></ul><p>因此，加噪的过程又可以表示为</p><script type="math/tex; mode=display">q(x_t|x_{t-1})=\mathcal{N}(x_t;\sqrt{\alpha_t}x_{t-1},(1-\alpha_t)I)\tag{2}</script><p>这里的$\alpha_t(t=1,2,…,T)$是预先定义好的超参数。</p><p>实际上，如果要这样一步一步采样$T$步得到$x_T$就太慢了，所以我们可以通过重参数化的技巧直接推导得到$x_0$一步到$x_T$的结果。采样$x_t\sim q(x_t|x_{t-1})$可以写作</p><script type="math/tex; mode=display">x_t=\sqrt{\alpha_t}x_{t-1}+\sqrt{1-\alpha_t}\epsilon_{t-1}^*, \epsilon_{t-1}^*\sim\mathcal{N}(\epsilon_{t-1}^*;0,I)\tag{3}</script><p>同样，采样$x_{t-1}\sim q(x_{t-1}|x_{t-2})$可以写作</p><script type="math/tex; mode=display">x_{t-1}=\sqrt{\alpha_{t-1}}x_{t-2}+\sqrt{1-\alpha_{t-1}}\epsilon_{t-1}^*, \epsilon_{t-2}^*\sim\mathcal{N}(\epsilon_{t-2}^*;0,I)\tag{4}</script><p>将它代入公式(3)，就能得到</p><script type="math/tex; mode=display">\begin{aligned}x_t&=\sqrt{\alpha_t\alpha_{t-1}}x_{t-2}+\sqrt{\alpha_t-\alpha_t\alpha_{t-1}}\epsilon_{t-2}^*+\sqrt{1-\alpha_t}\epsilon_{t-1}^*\\&=\sqrt{\alpha_t\alpha_{t-1}}x_{t-2}+\sqrt{1-\alpha_t\alpha_{t-1}}\epsilon_{t-2}\\&=...\\&=\sqrt{\bar{\alpha}_t}x_0+\sqrt{1-\bar{\alpha}_t}\epsilon_0\\&\sim\mathcal{N}(x_t;\sqrt{\bar{\alpha}_t}x_0,(1-\sqrt{1-\bar{\alpha}_t})I)\\&(\bar{\alpha}_t=\prod_{i=1}^{t}\alpha_t)\end{aligned}\tag{5}</script><p>其中第一行到第二行推导用到独立正态分布相加还是正态分布的性质，所以只需要计算新的正态分布的期望和方差。这个技巧对于扩散模型的理解非常重要，故在此记录下来。后面将看到，加噪对应了模型的训练过程，去噪（也叫做采样）对应了模型的推理过程。</p><p>采样是从$x_T$逐步去噪得到$x_0$的过程。当给定图像$x_t$，我们需要知道$x_{t-1}$的分布，从而采样得到$x_{t-1}$。根据DDPM的假设，我们有</p><script type="math/tex; mode=display">\begin{aligned}p(x_{t-1}|x_t)&=p(x_{t-1}|x_t,x_0)\\&=\frac{q(x_t|x_{t-1},x_0)q(x_{t-1}|x_0)}{q(x_t|x_0)}\\&=\frac{\mathcal{N}(x_t;\sqrt{\alpha_{t}}x_{t-1},(1-\alpha_t)I)\mathcal{N}(x_{t-1};\sqrt{\bar{\alpha}_{t-1}}x_0,(1-\bar{\alpha}_{t-1})I)}{\mathcal{N}(x_t;\sqrt{\bar{\alpha}_t}x_0,(1-\bar{\alpha}_t)I)}\\&\propto \exp\left\{-\left[\frac{(x_t-\sqrt{\alpha_t}x_{t-1})^2}{2(1-\alpha_t)}+\frac{(x_{t-1}-\sqrt{\bar{\alpha}_{t-1}}x_0)^2}{2(1-\bar{\alpha}_{t-1})}-\frac{(x_t-\sqrt{\bar{\alpha}_t}x_0)^2}{2(1-\bar{\alpha}_t)}\right]\right\}\\&=...(一系列推导)\\&\propto\mathcal{N}(x_{t-1};\underbrace{\frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})x_t+\sqrt{\bar{\alpha}_{t-1}}(1-\alpha_t)x_0}{1-\bar{\alpha}_t}}_{\mu_q(x_t,x_0)},\underbrace{\frac{(1-\alpha_t)(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}I}_{\Sigma_q(t)})\end{aligned}\tag{6}</script><p>由于$\Sigma_q(t)$由超参数组成，数值固定，故模型只需要预测$\mu_q(x_t,x_0)$就能得到$x_{t-1}$的分布。从构成上看，它由$x_t$和$x_0$共同决定，而$x_0$正是我们要求得的目标。我们可以训练一个神经网络，在很长一段时间里，扩散模型都是用的U-Net，用来预测一个$\hat{x}_0=\hat{x}_\theta(x_t,t)$，从而计算一个$\hat{\mu}_{t-1}=\mu_\theta(x_t,t)$（$\theta$是神经网络参数），再采样出$x_{t-1}$。从这里可以看出：</p><ul><li>神经网络并不是直接预测$\mu_q(x_t,x_0)$，而是通过预测它的参数来间接计算；</li><li>神经网络并不是预测出$x_0$后就直接作为生成的图像，而是先用它采样出$x_{t-1}$，再预测新的$x_0$，采样出$x_{t-2}$，循环下去直到得到$x_0$。</li></ul><p>这些操作都是为了让结果更可靠，例如迭代多次自然比一次出结果要靠谱。事实上，我们还可以通过预测$x_0$到$x_t$加噪时重参数化得到的$\epsilon$来计算$\mu_\theta(x_t,t)$。</p><h3 id="2-预测噪声的DDPM"><a href="#2-预测噪声的DDPM" class="headerlink" title="2. 预测噪声的DDPM"></a>2. 预测噪声的DDPM</h3><p>由公式(5)知</p><script type="math/tex; mode=display">x_t=\sqrt{\bar{\alpha}_t}x_0+\sqrt{1-\bar{\alpha}_t}\epsilon \tag{7}</script><p>那么</p><script type="math/tex; mode=display">x_0=\frac{x_t-\sqrt{1-\bar{\alpha}_t}\epsilon}{\sqrt{\alpha}_t}\tag{8}</script><p>代入$\mu_q(x_t,x_0)$得到</p><script type="math/tex; mode=display">\mu_q(x_t,x_0)=\frac{1}{\sqrt{\alpha}_t}x_t-\frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}\sqrt{\alpha_t}}\epsilon \tag{9}</script><p>用神经网络预测出$\hat{\epsilon}_\theta(x_t,t)$，得到</p><script type="math/tex; mode=display">\mu_{\theta}(x_t,t)=\frac{1}{\sqrt{\alpha}_t}x_t-\frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}\sqrt{\alpha_t}}\hat{\epsilon}_\theta(x_t,t)\tag{10}</script><p>从而也能得到$x_{t-1}$的分布。有研究发现预测噪声比预测$x_0$有更好的表现<font color="blue">[1]</font>。</p><h3 id="3-预测score-function的DDPM"><a href="#3-预测score-function的DDPM" class="headerlink" title="3. 预测score function的DDPM"></a>3. 预测score function的DDPM</h3><p>概率分布的对数导数被称为得分函数(Score Function)。用得分函数来理解DDPM的好处是，我们能看到扩散模型的几何意义。准确地说，是跟预测噪声的DPPM结合起来，理解去噪的几何意义。</p><p>要引入这种形式，要用到Tweedie公式。对于高斯随机变量$z\sim \mathcal{N}(z;\mu_z;\Sigma_z)$，Tweedie公式表示</p><script type="math/tex; mode=display">\mathbb{E}[\mu_z|z]=z+\Sigma_z\nabla_z\log{p(z)}</script><p>根据这个公式，我们能得到</p><script type="math/tex; mode=display">\mathbb{E}[\mu_x|x_t]=x_t+(1-\bar{\alpha}_t)\nabla_{x_t}\log{p(x_t)}\tag{11}</script><p>因为$\mu_{x_t}=\sqrt{\bar{\alpha}_t}x_0$，所以有</p><script type="math/tex; mode=display">\begin{aligned}\sqrt{\bar{\alpha}_t}x_0&=x_t+(1-\bar\alpha_t)\nabla\log{p(x_t)}\\x_0&=\frac{x_t+(1-\bar\alpha_t)\nabla\log{p(x_t)}}{\sqrt\alpha_t}\end{aligned}\tag{12}</script><p>代入$\mu_q(x_t,x_0)$并整理得到</p><script type="math/tex; mode=display">\mu_q(x_t,x_0)=\frac{1}{\alpha_t}x_t+\frac{1-\alpha_t}{\sqrt{\alpha_t}}\nabla\log{p(x_t)}\tag{13}</script><p>从而可以用神经网络去预测得分函数$\nabla\log{p(x_t)}$，记为$s_\theta(x_t,t)$，则有</p><script type="math/tex; mode=display">\mu_{\theta}(x_t,t)=\frac{1}{\sqrt{\alpha}_t}x_t+\frac{1-\alpha_t}{\sqrt{\alpha_t}}s_\theta(x_t,t)\tag{14}</script><h3 id="4-DDPM的几何意义"><a href="#4-DDPM的几何意义" class="headerlink" title="4. DDPM的几何意义"></a>4. DDPM的几何意义</h3><p>可以观察到公式(9)和公式(13)在结构上的相似性，我们建立一个等式</p><script type="math/tex; mode=display">\begin{aligned}x_0=\frac{x_t+(1-\bar\alpha_t)\nabla\log{p(x_t)}}{\sqrt{\bar\alpha_t}}&=\frac{x_t-\sqrt{1-\bar\alpha_t}\epsilon}{\sqrt{\bar\alpha_t}}\\\nabla\log{p(x_t)}&=-\frac{1}{\sqrt{1-\bar\alpha_t}}\epsilon\end{aligned}\tag{15}</script><p>可以看到得分函数和噪声，在常数倍数的缩放下，分别以一种相反的方向移动。从直觉上讲，这是非常自然的，所谓去噪，就是向着噪声相反的方向移动从而恢复原图。DDPM中得分函数的几何意义非常重要，有利于深入对扩散模型本身改进的研究。例如2022年的Flow Matching和2024年的Rectified Flow，就是把加噪和去噪的过程看成一个“流”<font color="blue">[11, 12]</font>。（<font color="red">虽然目前我还没整明白，感觉这是一个比较新鲜的方向</font>）</p><h3 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h3><p>到此为止，就介绍完了DDPM的基本思想和一些等价的形式。它是一个加噪-去噪模型，加噪时的噪声$\epsilon$作为标签来训练神经网络预测噪声的精度；去噪则在给定$x_T$的前提下，不断采样前一步时刻的图像，最终得到$x_0$。总结一下它的优缺点：</p><ol><li>相比传统的GAN模型，采用了新的生成范式，实现了更高质量的图像生成；</li><li>模型需要的参数量很小，即U-Net的参数量加上$\alpha_t$；</li><li>模型需要较多采样步骤才能得到较好的生成结果，需要在采样速度上优化；</li><li>模型是随机生成图像的，无法主动控制生成的方向。</li></ol><p>讲述DDPM用了不少篇幅，且涉及很多公式细节。我认为这是有必要的，因为这些细节正是扩散模型能取得成功的重要来源，后面所有扩散模型都是在它们的基础上演化的。接下来的讲述就主要提及核心思想，减少过于繁琐的细节推导了。</p><h2 id="二、加速采样：DDIM"><a href="#二、加速采样：DDIM" class="headerlink" title="二、加速采样：DDIM"></a>二、加速采样：DDIM</h2><p>DDIM (Denosing Diffusion Implicit Model)相比DDPM，训练过程相同，将采样效率提高了10-50倍。DDPM采样慢的原因是它是基于马尔科夫假设采样，需要较多步长构建原图。因此，DDIM对此的做法是：构建一个不依赖于一阶马尔科夫假设的采样分布$p(x_{t-1}|x_t,x_0)$，同时不影响DDPM中前向加噪的过程，即$q(x_t|x_0)=\mathcal{N}(x_t;\sqrt{\bar{\alpha}_t}x_0,(1-\bar{\alpha}_t)I)$。</p><p>假设采样分布依然是一个高斯分布，且均值也是关于$x_0,x_t$的线性函数，方差是时间步$t$的函数，那么</p><script type="math/tex; mode=display">p(x_{t-1}|x_t,x_0)=\mathcal{N}(x_{t-1};\lambda_t x_0+k_tx_t,\sigma_t^2I)\tag{16}</script><p>现在的问题是，怎样选取合适的$\lambda_t,k_t,\sigma_t$，使得$q(x_t|x_0)=\mathcal{N}(x_t;\sqrt{\bar{\alpha}_t}x_0,(1-\bar{\alpha}_t)I)$对$t=1,2,…,T$皆成立（注意$\alpha_t$依然是固定的超参数）。在$t=T$的时候，这个式子天然成立，因为$x_T$正是通过这个分布得到的。</p><p>假设在某个$t$的时候，该式子成立。如果找到一组合适的$\lambda_t^<em>$,$k_t^</em>$,$\sigma_t^*$，使得$t-1$时刻也成立，即$q(x_{t-1}|x_0)=\mathcal{N}(x_{t-1};\sqrt{\bar{\alpha}_{t-1}}x_0,(1-\bar{\alpha}_{t-1})I)$，自然根据数学归纳的思想，$t-2$到$1$的时刻的解的通项公式也就找到了。我们可以根据待定系数法进行求解。</p><p>根据公式(16)对$x_{t-1}$采样，得到</p><script type="math/tex; mode=display">x_{t-1}=\lambda_tx_0+k_tx_t+\sigma_t\epsilon_{t-1}^*\\ 其中\epsilon_{t-1}^*\sim\mathcal{N}(0,I)\tag{17}</script><p>根据$q(x_t|x_0)$的分布对$x_t$采样，得到</p><script type="math/tex; mode=display">x_t=\sqrt{\alpha}_tx_0+\sqrt{1-\bar{\alpha}_t}\epsilon_{t}^* \tag{18}</script><p>代入(17)有</p><script type="math/tex; mode=display">\begin{aligned}x_{t-1}&=(\lambda_t+k_t\sqrt{\bar{\alpha}_t})x_0+k\sqrt{1-\bar{\alpha}_t}\epsilon_t^*+\sigma_t\epsilon_{t-1}^* \\&=(\lambda_t+k_t\sqrt{\bar{\alpha}_t})x_0+\sqrt{k^2(1-\bar\alpha_t)+\sigma_t^2}\bar\epsilon_{t-1} \\&(再次用到了正态分布的可加性)\end{aligned} \tag{19}</script><p>正向采样时，根据$q(x_{t-1}|x_0)$的分布，得到</p><script type="math/tex; mode=display">x_{t-1}=\sqrt{\alpha}_{t-1}x_0+\sqrt{1-\bar{\alpha}_{t-1}}\epsilon_{t-1} \tag{20}</script><p>为了让两个方向采样的$x_{t-1}$分布一致，必须满足</p><script type="math/tex; mode=display">\begin{cases}\lambda_t+k\sqrt{\bar\alpha_t}=\sqrt{\bar\alpha_{t-1}}\\k_t^2(1-\bar\alpha_t)+\sigma_t^2=1-\bar\alpha_{t-1}\end{cases}\Rightarrow\left(\begin{matrix}\lambda_t^*\\k_t^*\\\sigma_t^*\end{matrix}\right)=\left(\begin{matrix}\sqrt{\bar\alpha_{t-1}}-\sqrt{\bar\alpha_t}\sqrt{\frac{1-\bar\alpha_{t-1}-\sigma_t^2}{1-\bar\alpha_t}}\\\sqrt{\frac{1-\bar\alpha_{t-1}-\sigma_t^2}{1-\bar\alpha_t}}\\\sigma_t\end{matrix}\right)\tag{21}</script><p>其中$\sigma_t$作为自由变量，方程有无数组解。将其代入(16)并整理得到DDIM的采样分布</p><script type="math/tex; mode=display">\begin{aligned}p(x_{t-1}|x_t,x_0)&=\mathcal{N}(x_{t-1};\sqrt{\bar\alpha_{t-1}}x_0+\sqrt{1-\bar\alpha_{t-1}-\sigma_t^2}\frac{x_t-\sqrt{\bar\alpha_t}x_0}{\sqrt{1-\bar\alpha_t}},\sigma_t^2I)\end{aligned}\tag{22}</script><p>改变$\sigma_t$就得到了不同的采样过程，同时前向加噪过程不变。采样中，深度神经网络发挥的作用，依然是预测加噪的噪声$\epsilon$。根据$x_t,x_0,\epsilon$的关系，可以求得上述正态分布的均值，从而能够采样$x_{t-1}$。值得注意的是：</p><ol><li>当$\sigma_t$跟DDPM中的一致时，这时的采样过程就是DDPM的采样过程；</li><li>当$\sigma_t=0$时，就将正态分布的均值作为采样的结果，采样过程完全确定，此时的生成模型是一个隐概率模型(Implicit Probabilistic Model)。DDIM也就是在这种情况下被命名的。</li></ol><p>现在知道了DDIM的采样分布，但并不知道DDIM是如何加速采样的。假设DDPM中需要从$t=T$依次采样到$t=1$，这个$T$往往较大，例如1000，那么就要采样1000次。DDIM的思路是，从原本的序列$L=[T,T-1,…,1]$中抽取一个长度远小于$T$的子序列$\tau=[\tau_s,\tau_{s-1},…\tau_{1}]$，在这个子序列中采样。例如$\tau_s=1000,\tau_{s-1}=950,\tau_{s-2}=900,…$。DDIM的论文<font color="blue">[3]</font>中显示，在CIFAR10上步长为50就达到了DDPM中步长为1000的效果（评价指标为FID, Fréchet Inception Distance）。</p><h2 id="三、类别控制：Classifier-Guidance"><a href="#三、类别控制：Classifier-Guidance" class="headerlink" title="三、类别控制：Classifier Guidance"></a>三、类别控制：Classifier Guidance</h2><p>到目前为止，扩散模型生成的图像都是随机的。类别控制(Classifier Guidance)<font color="blue">[4]</font>的引入，让它有了指定类别的生成能力，这是一切应用的始端。</p><h3 id="1-DDPM中基于条件的去噪过程"><a href="#1-DDPM中基于条件的去噪过程" class="headerlink" title="1. DDPM中基于条件的去噪过程"></a>1. DDPM中基于条件的去噪过程</h3><p>它的大致思路是，前向加噪过程和DDPM保持一致，在逆向采样过程中引入类别控制。假设类别标签为$y$，则带类别信号的采样过程定义为$\hat{p}(x_{t-1}|x_{t},y)$</p><script type="math/tex; mode=display">\begin{aligned}\hat{p}(x_{t-1}|x_{t},y)&=\frac{\hat{p}(x_{t-1},x_{t},y)}{\hat{p}(y|x_{t})\hat{p}(x_{t})}\\&=\frac{\hat{p}(y|x_{t-1},x_t)\hat{p}(x_{t-1}|x_t)\hat{p}(x_t)}{\hat{p}(y|x_t)\hat{p}(x_t)}\\&=\frac{\hat{p}(y|x_{t-1},x_t)\hat{p}(x_{t-1}|x_t)}{\hat{p}(y|x_t)}\\&=\frac{\hat{p}(y|x_{t-1})p(x_{t-1}|x_t)}{\hat{p}(y|x_t)}\end{aligned}\tag{23}</script><p>式中$\hat{p}(y|x_{t-1},x_t)=\hat{p}(y|x_{t-1})$可以理解为$x_{t-1}$比$x_t$更接近原图$x_0$，包含了$x_t$以及更多的信息量，可以只靠它决定$y$。$\hat{p}(x_{t-1}|x_t)=p(x_{t-1}|x_t)$意味着带类别信号的采样过程在不用类别信号的时候，跟DDPM的采样过程保持一致。当然，实际上没有这么理所当然，原文<font color="blue">[4]</font>的补充材料中有严谨的定义和推导。</p><p>式中容易处理的有$p(x_{t-1}|x_t)$和$\hat{p}(y|x_t)$，前者就是DDPM的去噪过程，后者可以训练一个关于$(x_t,y)$的分类器。但$\hat{p}(y|x_{t-1})$是不得而知的。作者将$\hat{p}(x_{t-1}|x_{t},y)$近似为扰动高斯分布(Perturbed Gaussian Distribution)，接下来将看到怎么操作的。</p><p>假设$p(x_{t-1}|x_{t})=\mathcal{N}(\mu,\Sigma)$，取其对数（默认底数为$e$）得到</p><script type="math/tex; mode=display">\log{p(x_{t-1}|x_{t})=\frac{1}{2}(x_{t-1}-\mu)^T\Sigma^{-1}(x_{t-1}-\mu)+C}\tag{24}</script><p>对于$\log{\hat{p}(y|x_{t-1})}$，作者假设其曲率比$\Sigma^{-1}$低（<font color="red">这里没搞懂是什么意思</font>），认为假设是合理的，因为当加噪步骤无限多的时候，$|\Sigma|\rightarrow 0$。在这种情况下，对$\log{\hat{p}(y|x_{t-1})}$在$x_{t-1}=\mu$处进行泰勒展开</p><script type="math/tex; mode=display">\begin{aligned}\log{\hat{p}(y|x_{t-1})}&\approx \log{\hat{p}(y|x_{t-1})}|_{x_{t-1}=\mu}+(x_{t-1}-\mu)\nabla_{x_{t-1}}\log{\hat{p}(y|x_{t-1})}|_{x_{t-1}=\mu}\\&=(x_{t-1}-\mu)g+C_1\\其中，g&=\nabla_{x_{t-1}}\log{\hat{p}(y|x_{t-1})}|_{x_{t-1}=\mu}，C_1是常数\end{aligned}\tag{25}</script><p>从而计算出</p><script type="math/tex; mode=display">\begin{aligned}\log{(\hat{p}(y|x_{t-1})p(x_{t-1}|x_{t}))}&=-\frac{1}{2}(x_{t-1}-\mu)^T\Sigma^{-1}(x_{t-1}-\mu)+(X_{t-1}-\mu)g+C_2 \\&=...\\&=-\frac{1}{2}(x_{t-1}-\mu-\Sigma g)^T\Sigma^{-1}(x_{t-1}-\mu-\Sigma g)+C_3\\&=\log{p(z)}+C_4\sim \mathcal{N}(\mu+\Sigma g,\Sigma)\end{aligned}\tag{26}</script><p>对比这个式子和公式(24)的形式，它们在形式上是一致的。不用担心常数，因为常数也会刚好匹配的。通过上述推导，得到了带类别条件的采样过程，还是用高斯分布来近似，只是均值要加上$\Sigma g$。$g$就是$t-1$时刻分类器输出的对数梯度，输入来自于神经网络预测的$\hat{\mu}_{t-1}=\mu_\theta({x_t},t)$和类别标签$y$。</p><h3 id="2-DDIM中基于条件的去噪过程"><a href="#2-DDIM中基于条件的去噪过程" class="headerlink" title="2. DDIM中基于条件的去噪过程"></a>2. DDIM中基于条件的去噪过程</h3><p>由于DDIM等确定性采样方法中设定了方差为0，上述推导不再适用。作者在研究中采用score-based的思路，利用了扩散模型和score matching之间的联系。首先根据贝叶斯公式</p><script type="math/tex; mode=display">\begin{aligned}p(x_t|y)&=\frac{p(y|x_t)p(x_t)}{p(y)}\\\log{p(x_t|y)}&=\log{p(y|x_t)}+\log{p(x_t)}-\log{p(y)}\\\nabla\log{p(x_t|y)}&=\nabla\log{p(y|x_t)}+\nabla\log{p(x_t)}\end{aligned}\tag{27}</script><p>然后需要定义条件下噪声的概念，把(15)中的公式写在下面</p><script type="math/tex; mode=display">\nabla\log{p(x_t)}=-\frac{1}{\sqrt{1-\bar\alpha_t}}\epsilon</script><p>这是无条件信号下的噪声，据此将有条件噪声$\hat\epsilon(x_t|y)$定义为</p><script type="math/tex; mode=display">\begin{aligned}\hat\epsilon(x_t|y)&:=-\sqrt{1-\bar\alpha}\nabla\log{p(x_t|y)}\\&=-\sqrt{1-\bar\alpha}\nabla\log{p(y|x_t)}-\sqrt{1-\bar\alpha}\nabla\log{p(x_t)}\\&=-\sqrt{1-\bar\alpha}\nabla\log{p(y|x_t)}+\epsilon(x_t)\\&=\epsilon(x_t)-\sqrt{1-\bar\alpha}\nabla\log{p(y|x_t)}\end{aligned}\tag{28}</script><p>只要将DDIM中预测的$\epsilon_\theta(x_t)$替换为$\hat{\epsilon}(x_t|y)$就得到了基于条件的去噪过程。还可以引入gradient score $s$来控制条件的强度，此时就将(28)改写为</p><script type="math/tex; mode=display">\hat\epsilon(x_t|y):=\epsilon(x_t)-s\sqrt{1-\bar\alpha}\nabla\log{p(y|x_t)}\tag{29}</script><h3 id="3-总结"><a href="#3-总结" class="headerlink" title="3. 总结"></a>3. 总结</h3><p>在扩散模型中引入类别控制是一大不错的创新，也是走向实用性的必然趋势。但这样扩散模型的效果好坏就很大程度上取决于分类器的好坏了，而且额外使用一个分类器也使得模型变得复杂。由此设想，能不能不用显式的分类器，而是让扩散模型在采样过程中就自然融入了类别信息，不仅简化了模型结构，还能提升标签信息的识别能力？这就是Classifier-free Guidance要解决的问题了。</p><h2 id="四、无分类器的控制：Classifier-free-Guidance"><a href="#四、无分类器的控制：Classifier-free-Guidance" class="headerlink" title="四、无分类器的控制：Classifier-free Guidance"></a>四、无分类器的控制：Classifier-free Guidance</h2><p>2021年分类器控制的扩散模型提出，2022年就出现了无分类控制(Classifier-free Guidance)的模型了<font color="blue">[5]</font>。其实它的思路很简单，在训练噪声预测模型的时候，直接在输入中引入类别信息，即$\hat\epsilon_\theta(x_t,t)\rightarrow\hat{\epsilon}_\theta(x_t,y,t)$。</p><h3 id="1-基本思路"><a href="#1-基本思路" class="headerlink" title="1. 基本思路"></a>1. 基本思路</h3><p>融入类别信息有很多种方法，现在比较常用的方法有直接拼接(Concatenation)、交叉注意力(Cross-Attention)、通道级注意力(Channel-wise Attention)。无论哪种方法，都要先将类别信息转化成向量，即需要一个文本编码器(Text Encoder)。在自然语言处理(NLP, Natural Language Processing)领域中，已经有很多成熟的方法可以使用了，例如Bert、T5、CLIP等，将文本转化成向量的过程叫做嵌入(Embedding)。而且不局限于类别信息，任何语义信息都可以转化成嵌入向量，所以Classifier-free的可用性大大提升，这项机制也成为了后面Dalle2、Stable Diffusion等模型的基础。</p><p>以交叉注意力为例，文本向量作为注意力中的$K$和$V$，图片表征(Image Representation)作为$Q$，注意力的输出为</p><script type="math/tex; mode=display">\text{Attention}(Q,K,V)=\text{softmax}(\frac{QK^T}{\sqrt{d_{k}}})V \tag{30}</script><h3 id="2-训练和采样"><a href="#2-训练和采样" class="headerlink" title="2. 训练和采样"></a>2. 训练和采样</h3><p>如果只用图文对去训练$\hat\epsilon_\theta(x_t,y,t)$，一是受限于图文对的数量，二是会束缚模型生成的多样性。所以，投入一定量的无条件信息的图片训练是恰当的，这时模型的标签输入为空，记为$\hat\epsilon_\theta(x_t,y=\varnothing,t)$。$y=\varnothing$的嵌入向量可以随机初始化，也可以人为给定。</p><p>采样公式可由(15)、(27)和(29)进一步推导，</p><script type="math/tex; mode=display">\begin{aligned}\underbrace{\hat\epsilon(x_t|y)}_{\hat\epsilon(x_t,y,t)}&=\underbrace{\epsilon(x_t)}_{\epsilon_\theta(x_t,y=\varnothing,t)}-s\sqrt{1-\bar\alpha}\nabla\log{p(y|x_t)}\\&=\epsilon(x_t)-s\sqrt{1-\bar\alpha}(\nabla\log{p(x_t|y)}-\nabla\log{p(y)})\\\hat\epsilon(x_t,y,t)&=\epsilon_\theta(x_t,y=\varnothing,t)+s(\epsilon_\theta(x_t,y,t)-\epsilon_\theta(x_t,y=\varnothing,t))\end{aligned}\tag{31}</script><p>最后一步计算用到了(15)中得分函数和噪声$\epsilon$的关系。于是，采样时跟DDPM、DDIM一样，只是需要将原本的$\hat\epsilon_\theta(x_t,t)$用$\hat\epsilon(x_t,y,t)$替代。</p><h2 id="五、隐空间加速推理：LDM"><a href="#五、隐空间加速推理：LDM" class="headerlink" title="五、隐空间加速推理：LDM"></a>五、隐空间加速推理：LDM</h2><p>此前扩散模型的扩散和采样过程都是在像素空间进行的，这样会造成很大的训练成本和推理开销。LDM (Latent Diffusion Model)提供了一种思路<font color="blue">[6]</font>，将图像转换到隐空间进行训练和推理，由于隐空间的参数量更小，从而减小了计算开销。像素空间和隐空间之间的转换用一个变分自编码器(VAE, Variational AutoEncoder)，将原始图像$x,x\in R^{H\times W\times 3}$通过VAE的编码器$\mathcal{E}$获得图片的隐空间表示$z,z\in R^{\frac{H}{f}\times\frac{W}{f}\times c}$，$f$为下采样率。扩散阶段就是从将原始图像的隐空间表示作为$z_0$，逐渐加噪得到$z_t$；去噪过程类似Classifier-free，在条件指导下逐步采样得到$z_0$，最后通过VAE的解码器$\mathcal{D}$重建像素空间的表征$x_0$。本质上LDM是一种二阶段的图像生成方法。后来知名的Stable Diffusion就是在LDM的基础上构建的。LDM的结构如<strong>图3</strong>所示。</p><p><img src="\images\blogs\diffusion\LDM模块图.png" alt="LDM结构"></p><center><p><b>图3</b> LDM结构示意</p></center><p>到此为止，主流的扩散模型的基本结构就已经构建完毕了，后面的大多数工作无非是在它的基础上对某些模块进行优化、加速或者应用。最繁琐的数学推导也都集中在了前期阶段（DDPM、DDIM、Classifier Guidance)，这是理论孕育的阶段。到后面，就是应用百花齐放的阶段了。</p><h2 id="六、扩散模型的应用"><a href="#六、扩散模型的应用" class="headerlink" title="六、扩散模型的应用"></a>六、扩散模型的应用</h2><p>2022年Dalle2诞生，掀起了一片互联网的浪花。它的功能很简单，正如Classifier-free描述的那样，用语义信息控制图像生成，因为效果非常好，受到大家关注。它利用CLIP<font color="blue">[8]</font>将图像和文本信息编码到同一表征空间进行训练和推理<font color="blue">[7]</font>。如今已经有Dalle3，但是OpenAI并没有将它开源，只是提出了一种re-caption的方法<font color="blue">[9]</font>，对数据进行预处理。（OpenAI真是越来越往CloseAI的方向靠拢了）</p><p>Stable Diffusion是基于LDM基础上的直接应用，如今已经发展到了Stable Diffusion 3<font color="blue">[12]</font>。它提出了一种修正流(Rectified Flow)，这就不得不提到一篇叫流匹配的工作(Flow Matching)<font color="blue">[11]</font>，将扩散模型建模成一个流的过程(<font color="red">这个还在学习中，不太懂</font>)。不得不感叹数学和物理在扩散模型发展中的重要性，我还看到一个叫薛定谔的桥的方法<font color="blue">[13]</font>，突然就看不明白了。不过Stable Diffusion 3还有个重要的价值，就是证明了DiT (Diffusion Transformer)的价值<font color="blue">[10]</font>，这是用transformer替代扩散模型中长期使用的U-Net。transformer真是有一统三界的趋势。</p><p>此外，我还看到了扩散模型在图像编辑方面的应用，即给定一张图片和指令，按指令修改图片<font color="blue">[14, 15]</font>。</p><h2 id="七、可能的研究前景"><a href="#七、可能的研究前景" class="headerlink" title="七、可能的研究前景"></a>七、可能的研究前景</h2><p>从前面的介绍来看，在这里直接列出一些可以研究的方向：</p><ol><li>加速采样，就像DDIM那样对DDPM的采样进行优化，加快图像生成速度；</li><li>优化压缩模型，即原始图像空间和潜在空间的VAE模型，以更小的潜在空间维度生成更高质量的图像；</li><li>语义信息编码，或者说如何让模型充分理解语义信息（这里或许可以结合LLM）；</li><li>网络架构，例如从U-Net到DiT；</li><li>应用扩展，如视频生成和图像编辑等。</li></ol><p><br></p><h4 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h4><p>[1] Luo, C. (2022). Understanding diffusion models: A unified perspective. <em>arXiv preprint arXiv:2208.11970</em>.</p><p>[2] Ho, J., Jain, A., &amp; Abbeel, P. (2020). Denoising diffusion probabilistic models. <em>Advances in neural information processing systems</em>, <em>33</em>, 6840-6851.</p><p>[3] Song, J., Meng, C., &amp; Ermon, S. (2020). Denoising diffusion implicit models. <em>arXiv preprint arXiv:2010.02502</em>.</p><p>[4] Dhariwal, P., &amp; Nichol, A. (2021). Diffusion models beat gans on image synthesis. <em>Advances in neural information processing systems</em>, <em>34</em>, 8780-8794.</p><p>[5] Ho, J., &amp; Salimans, T. (2022). Classifier-free diffusion guidance. <em>arXiv preprint arXiv:2207.12598</em>.</p><p>[6] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., &amp; Ommer, B. (2022). High-resolution image synthesis with latent diffusion models. In <em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em> (pp. 10684-10695).</p><p>[7] Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., &amp; Chen, M. (2022). Hierarchical text-conditional image generation with clip latents. <em>arXiv preprint arXiv:2204.06125</em>, <em>1</em>(2), 3.</p><p>[8] Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., … &amp; Sutskever, I. (2021, July). Learning transferable visual models from natural language supervision. In <em>International conference on machine learning</em> (pp. 8748-8763). PMLR.</p><p>[9] Betker, J., Goh, G., Jing, L., Brooks, T., Wang, J., Li, L., … &amp; Ramesh, A. (2023). Improving image generation with better captions. <em>Computer Science. <a href="https://cdn.openai.com/papers/dall-e-3.pdf">https://cdn.openai.com/papers/dall-e-3.pdf</a></em>, <em>2</em>(3), 8.</p><p>[10] Peebles, W., &amp; Xie, S. (2023). Scalable diffusion models with transformers. In <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em> (pp. 4195-4205).</p><p>[11] Lipman, Y., Chen, R. T., Ben-Hamu, H., Nickel, M., &amp; Le, M. (2022). Flow matching for generative modeling. <em>arXiv preprint arXiv:2210.02747</em>.</p><p>[12] Esser, P., Kulal, S., Blattmann, A., Entezari, R., Müller, J., Saini, H., … &amp; Rombach, R. (2024, March). Scaling rectified flow transformers for high-resolution image synthesis. In <em>Forty-first International Conference on Machine Learning</em>.</p><p>[13] Shi, Y., De Bortoli, V., Campbell, A., &amp; Doucet, A. (2024). Diffusion Schrödinger bridge matching. <em>Advances in Neural Information Processing Systems</em>, <em>36</em>.</p><p>[14] Brooks, T., Holynski, A., &amp; Efros, A. A. (2023). Instructpix2pix: Learning to follow image editing instructions. In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (pp. 18392-18402).</p><p>[15] Sheynin, S., Polyak, A., Singer, U., Kirstain, Y., Zohar, A., Ashual, O., … &amp; Taigman, Y. (2024). Emu edit: Precise image editing via recognition and generation tasks. In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (pp. 8871-8879).</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;2021年以来，生成式模型扛把子diffusion model红红火火，已成为当代人工智能研究者不得不面对的话题。本人作为初学者，最近学习了一些经典扩散模型如DDPM、DDIM、Classifier Guidance等，以及一些较新的模型如Stable Diffusion </summary>
      
    
    
    
    
    <category term="diffusion" scheme="https://overrideturing.github.io/tags/diffusion/"/>
    
  </entry>
  
  <entry>
    <title>欢迎来到我的世界！！！Surprise, Yumou!</title>
    <link href="https://overrideturing.github.io/2024/11/04/hello-world/"/>
    <id>https://overrideturing.github.io/2024/11/04/hello-world/</id>
    <published>2024-11-03T16:17:57.145Z</published>
    <updated>2024-11-03T17:32:19.885Z</updated>
    
    <content type="html"><![CDATA[<p>你一定想不到我是如何弄了这么个玩意儿的，这可是面试加分项！全靠下面这个：<a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;你一定想不到我是如何弄了这么个玩意儿的，这可是面试加分项！全靠下面这个：&lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/doc</summary>
      
    
    
    
    
  </entry>
  
</feed>
