{"meta":{"title":"Welcome to my blog!","subtitle":"","description":"","author":"OverrideTuring","url":"https://overrideturing.github.io","root":"/"},"pages":[],"posts":[{"title":"泛函分析重要问题记录","slug":"泛函分析重要问题记录","date":"2024-11-04T06:32:08.000Z","updated":"2024-11-04T06:55:00.393Z","comments":true,"path":"2024/11/04/泛函分析重要问题记录/","permalink":"https://overrideturing.github.io/2024/11/04/%E6%B3%9B%E5%87%BD%E5%88%86%E6%9E%90%E9%87%8D%E8%A6%81%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/","excerpt":"","text":"距离空间 证明：Arzela定理：$C[a,b]$中的子集$A$是列紧的，当且仅当$A$中的函数是一致有界和等度连续的. 即存在$K&gt;0$，使得对于每一点$t\\in [a,b]$及一切$x\\in A$，有$\\vert x(t)\\vert \\leq K$，并且对于任意的$\\varepsilon &gt; 0$，存在$\\delta &gt; 0$，当$\\vert t_1 - t_2 \\vert &lt; \\delta$时，$\\vert x(t_1)-x(t_2)\\vert &lt; \\varepsilon (\\forall x\\in A)$. 证明：距离空间$(X,d)$上的压缩映射原理（也叫Banach不动点定理）。 证明：任何距离空间都可以完备化。（同理可证：任何赋范线性空间都可以完备化） 赋范空间 证明：$L^p(E)(1\\leq p &lt; \\infty)$是Banach空间（即完备的赋范空间）。 证明：$L^p[a,b]$是可分的（具有可数稠密子集）。 证明：$L^\\infty(E)$是不可分的Banach空间。 证明：对于任意的$x(t)\\in L^\\infty(E),m(E)&lt;\\infty$，有$\\Vert x\\Vert_p \\rightarrow \\Vert x\\Vert_\\infty$，即 \\lim_{p \\rightarrow +\\infty}\\left(\\int_E\\vert x(t)\\vert^p dt\\right)^\\frac{1}{p}=\\Vert x\\Vert_\\infty证明如下： 由\\Vert x\\Vert_\\infty定义可知，\\forall \\varepsilon >0，存在集合E_1 \\subset E，m(E_1)>0，\\\\ s.t. \\forall t\\in E_1，\\Vert x\\Vert_\\infty - \\varepsilon \\leq x(t)\\leq \\Vert x\\Vert_\\infty \\\\ 则\\left(\\int_{E_1}(\\Vert x\\Vert_\\infty - \\varepsilon)^p dt\\right)^\\frac{1}{p} \\leq \\left(\\int_E\\vert x(t)\\vert^p dt\\right)^\\frac{1}{p} \\leq \\left(\\int_E\\Vert x\\Vert_\\infty^p dt\\right)^\\frac{1}{p}\\\\ 即(\\Vert x\\Vert_\\infty - \\varepsilon)m(E_1)^\\frac{1}{p} \\leq \\left(\\int_E\\vert x(t)\\vert^p dt\\right)^\\frac{1}{p} \\leq \\Vert x\\Vert_\\infty m(E)^\\frac{1}{p}\\\\ 对不等式取极限有\\Vert x\\Vert_\\infty - \\varepsilon \\leq \\lim_{p \\rightarrow +\\infty}\\left(\\int_E\\vert x(t)\\vert^p dt\\right)^\\frac{1}{p} \\leq \\Vert x\\Vert_\\infty\\\\ 由\\varepsilon 的任意性可得\\lim_{p \\rightarrow +\\infty}\\left(\\int_E\\vert x(t)\\vert^p dt\\right)^\\frac{1}{p}=\\Vert x\\Vert_\\infty 证明：赋范线性空间$X$的一个子空间$X_1$是开集，则$X_1=X$。 证明：Riese引理：设$(X,\\Vert \\cdot \\Vert)$是一个赋范空间，$X_0$是$X$的真闭子空间，则对于$\\forall\\varepsilon &gt;0$，存在$x_0\\in X$，使得$\\Vert x_0 \\Vert=1$，且对于$\\forall x\\in X_0$， \\Vert x-x_0\\Vert > 1-\\varepsilon. 证明：任意实的$n$维赋范空间必与$R^n$代数同构，拓扑同胚。 证明：赋范空间是有限维的当且仅当$X$中的任何有界集是列紧的。 证明：设$X$是Banach空间，$M$是$X$的闭子空间，则赋范空间$X$关于$M$的商空间$X/M$是Banach空间。 内积空间 证明：Schwarz不等式：设$H$是内积空间，对于$\\forall x,y\\in H$，有 \\Vert (x,y)\\Vert^2 \\leq (x,x)(y,y). 证明：正交列完备的条件是它是最大的正交列，此时Parseval等式成立（四个等价命题）。 线性算子和线性泛函 证明：线性算子连续则必有界，有界则必连续。 证明：设$(X,\\Vert \\cdot \\Vert)$是有限维赋范空间，$(Y,\\Vert \\cdot \\Vert)$是任意一个赋范空间，$T$是从$X$到$Y$的线性算子，则$T$是有界线性算子。 证明：Baire纲定理：完备的距离空间是第二纲集。（即不能由至多可数个疏集的并表示） 证明：一致有界原则：设$\\{T_\\alpha | \\alpha \\in I \\}$是Banach空间$X$上到赋范空间$X_1$中的有界线性算子族，如果对于$\\forall x \\in X$，有 \\sup_\\alpha \\Vert T_\\alpha x\\Vert < \\infty, 则$\\{T_\\alpha | \\alpha \\in I \\}$是有界集。 证明：开映射定理：Banach空间的有界线性算子将开集映射成开集。 证明：逆算子定理：设$T$是从Banach空间$X$上到Banach空间$X_1$上的一对一的有界线性算子，则$T$的逆算子存在，且$T^{-1}$是有界的。 证明：闭图像定理：设$T$是Banach空间$X$上到Banach空间$X_1$中的闭线性算子，则$T$是有界线性算子。 证明：复的Hahn-Banach定理：设$X$是一个复的赋范空间，$G$是$X$的子空间，$f$是$G$上的有界线性泛函，则$f$可以保持范数不变地延拓到全空间$X$上。 即存在$X$上的有界线性泛函$F$，使得 (1) 对于$\\forall x\\in G,F(x)=f(x)$； (2) $\\Vert F \\Vert = \\Vert f \\Vert_G$， 其中$\\Vert f \\Vert_G$表示$f$作为$G$上的有界线性泛函的范数。 提示：证明过程会用到Zorn引理，设$P$是非空半序集，如果$P$中任何全序子集都有上界，则$P$至少有一个极大元。 共轭空间和共轭算子 证明：$f$是$L^p[a,b]$上的有界线性泛函，则存在唯一的$y(t)\\in L^q[a,b]$，$(\\frac{1}{p}+\\frac{1}{q}=1)$，使得 f(x)=\\int^b_a x(t)y(t)dt, \\forall x \\in L^p[a,b], 且 \\Vert f \\Vert = \\Vert y \\Vert_q = (\\int_a^b \\vert y(t) \\vert^q dt)^\\frac{1}{q}.反之，对于$\\forall y \\in L^q[a,b]$，上式定义了$L^p[a,b]$上的一个有界线性泛函。（注意：此定理证明要用到大量实变函数定理） 证明：Riesz表示定理：设$H$是一个Hilbert空间，$f$是$H$上的有界线性泛函，则存在唯一的$y_f \\in H$，使得 f(x)=(x,y_f), \\forall x \\in H,并且 \\Vert f \\Vert = \\Vert y_f \\Vert.（提示：在该定理证明过程中，应该深刻理解维度的概念，感受泛函分析的美妙） 证明：$L^p(1&lt;p&lt;\\infty)$是自反的，$l^p(1&lt;p&lt;\\infty)$是自反的。 证明：如果$\\{x_n\\} $弱收敛，则$\\{\\Vert x_n \\Vert\\}$有界。（提示：用一致有界原则，体会数学思想） 证明：设$X$是Banach空间，$T\\in B(X)$，则$\\sigma(T)$是有界集。 证明：设$T$是Banach空间$X$到$X$的有界线性算子，$\\lambda \\in \\rho(T)$，且$\\vert \\mu \\vert &lt; \\Vert (\\lambda I - T)^{-1}\\Vert^{-1}$，则$\\lambda + \\mu \\in \\rho(T)$，即$\\rho(T)$是一个开集。 证明：设$T$是有界线性算子，则$\\sigma(T)\\neq \\phi$。（注意：此定理难度超高，立足的空间来去变换，如若掌握证明技巧，对抽象思维大有裨益） 证明：设$T\\in B(X)$，则$r_\\sigma(T)=\\sup_{\\lambda \\in \\sigma\\{T\\}} \\vert \\lambda \\vert$。 谱理论 证明：设$T$是Hilbert空间$H$上的自共轭算子，则$T$的剩余谱是空集。（提示：用到共轭算子零空间、正交、值域所具有的一些性质） 证明：设$H$是Hilbert空间，线性算子$T\\in B(H)$是紧的，当且仅当存在$B(H)$中的一列有穷秩算子$T_n$，使得$\\Vert T-T_n\\Vert \\rightarrow 0(n\\rightarrow\\infty)$。 证明：设$X$是Banach空间，$T_1,T_2\\in B(X)$。则： (1) $T_1$和$T_2$中有一个是紧的，那么$T_1T_2$是紧的。 (2) $T\\in B(X)$，$T$是紧的当且仅当$T^*$是紧的。 证明：恒等算子$I:X\\rightarrow X$是紧的，当且仅当$\\dim{X} &lt; \\infty$。（提示：用Riesz引理） 证明：设$T$是赋范空间$X$到$X$的紧线性算子，那么对于$\\forall a&gt;0$，$T$的特征值$\\lambda$满足$\\vert \\lambda \\vert &gt; \\alpha$的个数是有限的。 证明：令$T$是从赋范空间$X$到$X$的紧线性算子，若$\\lambda \\neq 0$，则$\\lambda I-T$的零空间$N(\\lambda I-T)$是有限维的。 证明：设$T$是从Banach空间$X$到$X$的紧线性算子，$\\lambda \\neq 0$，则$R(\\lambda I-T)$是闭的。 证明：$X$是Banach空间，$T\\in K(X)$，$\\lambda \\neq 0$，若$N(\\lambda I-T)=\\{0\\}$，则$R(\\lambda I-T)=X$。 证明：设$T$是从Hilbert空间$H$到$H$的紧线性算子，$\\lambda \\neq 0$，则 H=N(\\bar{\\lambda}-T^*)\\oplus R(\\lambda I-T). 证明：Fredholm二择一定理：设$T$是从Hilbert空间$H$到$H$的紧线性算子，$\\lambda \\neq 0$，则： (i) 对于任意的$y\\in H$，非齐次方程 (\\lambda I-T)x=y \\tag{1} 有唯一解的充要条件是齐次方程 (\\lambda I-T)x=0 \\tag{2} 没有非零解。 (ii) 若方程(2)有非零解， 非齐次方程(1)有解的充要条件是$y$与方程 (\\bar{\\lambda}I-T^*)x=0 \\tag{3} 的所有解正交，即：$y\\perp N(\\bar{\\lambda}I-T^*)$。","categories":[],"tags":[{"name":"mathematic","slug":"mathematic","permalink":"https://overrideturing.github.io/tags/mathematic/"}]},{"title":"实变函数重要问题记录","slug":"实变函数重要问题记录","date":"2024-11-04T06:32:00.000Z","updated":"2024-11-04T08:43:00.428Z","comments":true,"path":"2024/11/04/实变函数重要问题记录/","permalink":"https://overrideturing.github.io/2024/11/04/%E5%AE%9E%E5%8F%98%E5%87%BD%E6%95%B0%E9%87%8D%E8%A6%81%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/","excerpt":"","text":"证明：康托尔-伯恩斯坦(Cantor-Bernstein)定理：对于集合A和B，存在单射$f:A\\rightarrow B$和$g:B\\rightarrow A$，则存在一个双射$h:A\\rightarrow B$。（这样两个集合就等势） 证明：一个集合为无限集当且仅当它和自己的一个真子集可建立一一对应关系。 证明：在区间[0,1]内，不存在这样的函数，在有理数点处连续，在无理数点处不连续。 备注：反过来是成立的，即存在一种函数，在有理数点处不连续，在无理数点处连续，例如黎曼函数 f(x)=\\left\\{\\begin{matrix} p/q,&x=p/q，p,q都属于正整数，p/q为既约真分数 \\\\ 0,&x为无理数 \\end{matrix}\\right.\\\\ x\\in [0,1]实际上有一个更通用性的结论，定义在一个区间上的函数在某个稠密子集上连续，那么就找不到另一个函数，使得它们的不连续点互补。 证明：Carathéodory定理说明了什么样的集合是可测集，它的等价定义是，一个集合E是可测集，那么对于任意$\\varepsilon &gt; 0$，存在一个闭集$F\\subset E$，使得$m^*(E-F)&lt;\\varepsilon$。同样，还有两种闭集的等价定义，参看川大实变函数课程P35。 证明：有两个Lebesgue可测函数$f$和$g$，$f+g$也是勒贝格可测函数。 证明：可测函数三大定理：Egorov定理（几乎一致收敛）、Riesz定理（子列几乎收敛）、Lusin定理（依测度收敛于连续函数） 重点概念： 几乎处处收敛：指去掉一个零测集后，数列$\\{f_k(x)\\}$处处收敛于$f(x)$ 依测度收敛：数列$\\{f_k(x)\\}$中不收敛于$f(x)$的点集在极限情况下是零测集 （这两者并不等价，也不存在包含关系，函数序列可能处处不收敛，但依测度收敛，一种情况是，不收敛的点在变少，但在整个区间可能是浮动存在的） 证明：有关可积函数的定理：Lebesgue控制收敛定理、Levi定理（勒贝格单调收敛定理）、Fatou引理、Fubini定理 证明：Lebesgue定理：有界函数黎曼可积的充要条件是几乎处处连续。 证明：积分全连续性：在E上可积，对于任意$\\varepsilon&gt;0$，存在$\\delta&gt;0$，如果$E’\\subset E$，且$m(E’) &lt; \\delta$，则$\\int_E f(x)dx &lt; \\varepsilon$。 证明：全连续函数（不是积分全连续性）是有界变差函数，有界变差函数等价于两个单调递增函数的差。 证明：Lebesgue积分意义下的牛顿-莱布尼茨公式（参看川大实变函数课程P43，网课未给出完整证明） 证明：存在处处连续处处不可导的函数（构造式证明） 证明：rising sun lemma（参考川大实变函数课程P44） 设f:R\\rightarrow R是连续函数， S=\\left\\{x \\in R | \\exists y>x, s.t. f(x)1,q>1)\\\\ 移项即得\\left[\\int_a^b \\vert f(x)+g(x)\\vert ^p dx \\right]^\\frac{1}{p} \\leq \\left[\\int_a^b \\vert f(x)\\vert ^p dx\\right]^\\frac{1}{p} + \\left[\\int_a^b \\vert g(x)\\vert ^p dx\\right]^\\frac{1}{p} 证明：Holder不等式 如果\\frac{1}{p}+\\frac{1}{q}=1 (p,q>0)，则\\Vert fg\\Vert_1 \\leq \\Vert f\\Vert_p \\Vert g\\Vert_q \\\\ 提示：已知Young不等式ab\\leq \\frac{a^p}{p}+\\frac{b^q}{q} (a,b>0)\\\\证明如下： \\int_a^b\\frac{\\vert f(x)g(x)\\vert}{\\Vert f\\Vert_p \\Vert g\\Vert_q} dx \\leq \\int_a^b\\frac{1}{p}\\frac{\\vert f(x)\\vert^p}{\\Vert f\\Vert_p^p}+ \\frac{1}{q}\\frac{\\vert g(x)\\vert^q}{\\Vert f\\Vert_g^g}dx =\\frac{1}{p}+\\frac{1}{q}=1 数学公式渲染太让人头疼，可以参考这个： 如何在hexo中支持Mathjax 此外，把node_modules/hexo-renderer-mathjax/mathjax.html中的某script标签的src改为https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML","categories":[],"tags":[{"name":"mathematic","slug":"mathematic","permalink":"https://overrideturing.github.io/tags/mathematic/"}]},{"title":"Diffusion发展脉络","slug":"diffusion发展脉络","date":"2024-11-03T17:36:48.000Z","updated":"2024-11-04T12:47:29.748Z","comments":true,"path":"2024/11/04/diffusion发展脉络/","permalink":"https://overrideturing.github.io/2024/11/04/diffusion%E5%8F%91%E5%B1%95%E8%84%89%E7%BB%9C/","excerpt":"","text":"2021年以来，生成式模型扛把子diffusion model红红火火，已成为当代人工智能研究者不得不面对的话题。本人作为初学者，最近学习了一些经典扩散模型如DDPM、DDIM、Classifier Guidance等，以及一些较新的模型如Stable Diffusion 3和Flow Matching等。趁着记忆犹新赶紧记录下来。本文梳理了diffusion model的发展脉络，并展望了未来diffusion model的发展方向。 开山之作：DDPM2020年以前，在生成模型(Generative Model)领域占统治地位的一直是GANs(Generative Adversarial Networks)。随后，一种去噪扩散概率模型DDPM (Denoising Diffusion Probabilistic Model)横空出世，凭借较小的参数量就能取得很好的图像生成效果，而且由于是概率模型，生成图像的多样性也能得到保证。因此，从2021年至今，扩散模型的研究就如雨后春笋般涌现。一条大致的发展路线如图1所示。从最开始随机生成图像的DDPM，到最后能根据自然语言prompt生成各种风格图像的Stable Diffusion，发展是显著且迅速的。在接下来的介绍中，会看到这样的成就并非一蹴而就，每一步工作都严格建立在前人的基础之上，最后逐渐累积到今天的成果。 flowchart LR A[DDPM] B[DDIM] C[Classifier Guidance] D[Classifier-free Guidance] E[Latent Diffusion Model] F[Dalle2] G[Stable Diffusion] H[Flow Matching] I[Rectified Flow] A--加速采样过程-->B--分类器指导-->C--隐式条件指导-->D--隐空间推理-->E E-->F E-->G A--不同理解视角-->H--流的改进-->I 图1 扩散模型大致发展脉络 首先，要从开山之作DDPM说起，它把图像生成建模成去噪过程。假设有一张图像$x_0$，对它逐渐施加噪声得到一系列图像$x_1,x_2,…,x_T$。图像变得越来越模糊，直到施加$T$次噪声后，图像彻底变成标准正态分布$p(x_T)=\\mathcal{N}(x_T;0,I)$。图像生成则是逆过程，从一张噪声图像开始，逐渐去噪得到原始图像。以上过程如图2所示。 图2 DDPM过程示意（从左到右是加噪，从右到左是去噪） 加噪的过程可以表示为 q(x_{1:T}|x_0)=\\prod_{t=1}^Tq(x_t|x_{t-1}) \\tag{1}这是一个随机过程。DDPM基于以下假设： 它是马尔科夫随机过程，所以每个$x_t(t\\neq 0)$仅由$x_{t-1}$决定； 每个中间图像都符合正态分布，它的均值取决于上一步的输出图像。 因此，加噪的过程又可以表示为 q(x_t|x_{t-1})=\\mathcal{N}(x_t;\\sqrt{\\alpha_t}x_{t-1},(1-\\alpha_t)I) \\tag{2}这里的$\\alpha_t(t=1,2,…,T)$是预先定义好的超参数。 实际上，如果要这样一步一步采样$T$步得到$x_T$就太慢了，所以我们可以通过重参数化的技巧直接推导得到$x_0$一步到$x_T$的结果。采样$x_t\\sim q(x_t|x_{t-1})$可以写作 x_t=\\sqrt{\\alpha_t}x_{t-1}+\\sqrt{1-\\alpha_t}\\epsilon_{t-1}^*, \\epsilon_{t-1}^*\\sim\\mathcal{N}(\\epsilon_{t-1}^*;0,I) \\tag{3}同样，采样$x_{t-1}\\sim q(x_{t-1}|x_{t-2})$可以写作 x_{t-1}=\\sqrt{\\alpha_{t-1}}x_{t-2}+\\sqrt{1-\\alpha_{t-1}}\\epsilon_{t-1}^*, \\epsilon_{t-2}^*\\sim\\mathcal{N}(\\epsilon_{t-2}^*;0,I) \\tag{4}将它代入公式(3)，就能得到 \\begin{aligned} x_t&=\\sqrt{\\alpha_t\\alpha_{t-1}}x_{t-2}+\\sqrt{\\alpha_t-\\alpha_t\\alpha_{t-1}}\\epsilon_{t-2}^*+\\sqrt{1-\\alpha_t}\\epsilon_{t-1}^*\\\\ &=\\sqrt{\\alpha_t\\alpha_{t-1}}x_{t-2}+\\sqrt{1-\\alpha_t\\alpha_{t-1}}\\epsilon_{t-2}\\\\ &=...\\\\ &=\\sqrt{\\bar{\\alpha}_t}x_0+\\sqrt{1-\\bar{\\alpha}_t}\\epsilon_0\\\\ &\\sim\\mathcal{N}(x_t;\\sqrt{\\bar{\\alpha}_t}x_0,(1-\\sqrt{1-\\bar{\\alpha}_t})I) \\end{aligned}\\tag{5}其中第一行到第二行推导用到独立正态分布相加还是正态分布的性质，所以只需要计算新的正态分布的期望和方差，就能得到分布。这个技巧对于扩散模型的理解非常重要，故在此记录下来。后面将看到，加噪对应了模型的训练过程，去噪（也叫做采样）对应了模型的推理过程。 采样是从$x_T$逐步去噪得到$x_0$的过程。当给定图像$x_t$，我们需要知道$x_{t-1}$的分布，从而采样得到$x_{t-1}$。根据DDPM的假设，我们有 \\begin{aligned} q(x_{t-1}|x_t)&=q(x_{t-1}|x_t,x_0)\\\\ &=\\frac{q(x_t|x_{t-1},x_0)q(x_{t-1}|x_0)}{q(x_t|x_0)}\\\\ &=\\frac{\\mathcal{N}(x_t;\\sqrt{\\alpha_{t}}x_{t-1},(1-\\alpha_t)I) \\mathcal{N}(x_{t-1};\\sqrt{\\bar{\\alpha}_{t-1}}x_0,(1-\\bar{\\alpha}_{t-1})I)} {\\mathcal{N}(x_t;\\sqrt{\\bar{\\alpha}_t}x_0,(1-\\bar{\\alpha}_t)I)}\\\\ &\\propto \\exp\\left\\{ -\\left[ \\frac{(x_t-\\sqrt{\\alpha_t}x_{t-1})^2}{2(1-\\alpha_t)} +\\frac{(x_{t-1}-\\sqrt{\\bar{\\alpha}_{t-1}}x_0)^2}{2(1-\\bar{\\alpha}_{t-1})} -\\frac{(x_t-\\sqrt{\\bar{\\alpha}_t}x_0)^2}{2(1-\\bar{\\alpha}_t)} \\right] \\right\\}\\\\ &=...(一系列推导)\\\\ &\\propto\\mathcal{N}(x_{t-1}; \\underbrace{\\frac{\\sqrt{\\alpha_t}(1-\\bar{\\alpha}_{t-1})x_t+\\sqrt{\\bar{\\alpha}_{t-1}}(1-\\alpha_t)x_0}{1-\\bar{\\alpha}_t}}_{\\mu_q(x_t,x_0)}, \\underbrace{\\frac{(1-\\alpha_t)(1-\\bar{\\alpha}_{t-1})}{1-\\bar{\\alpha}_t}I}_{\\Sigma_q(t)}) \\end{aligned}\\tag{6}由于$\\Sigma_q(t)$由超参数组成，数值固定，故模型只需要预测$\\mu_q(x_t,x_0)$就能得到$x_{t-1}$的分布。从构成上看，它由$x_t$和$x_0$共同决定，而$x_0$正是我们要求得的目标。我们可以训练一个神经网络，在很长一段时间里，扩散模型都是用的U-Net，用来预测$x_0$，从而计算一个$\\hat{\\mu}(x_t,t)$，再采样出$x_{t-1}$。从这里可以看出： 神经网络并不是直接预测$\\mu_q(x_t,x_0)$，而是通过预测它的参数来间接计算； 神经网络并不是预测出$x_0$后就直接作为生成的图像，而是先用它采样出$x_{t-1}$，再预测新的$x_0$，采样出$x_{t-2}$，循环下去直到得到$x_0$。 这些操作都是为了让结果更可靠，例如迭代多次自然比一次出结果要靠谱。事实上，我们还可以通过预测$x_0$到$x_t$加噪时重参数化得到的$\\epsilon$。由公式(5)知 x_t=\\sqrt{\\bar{\\alpha}_t}x_0+\\sqrt{1-\\bar{\\alpha}_t}\\epsilon \\tag{7}那么 x_0=\\frac{x_t-\\sqrt{1-\\bar{\\alpha}_t}\\epsilon}{\\sqrt{\\alpha}_t}\\tag{8}代入$\\mu_q(x_t,x_0)$得到 \\mu_q(x_t,x_0)=\\frac{1}{\\sqrt{\\alpha}_t}x_t-\\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}\\sqrt{\\alpha_t}}\\epsilon \\tag{9}用神经网络预测出$\\hat{\\epsilon}_\\theta(x_t,t)$（$\\theta$是神经网络参数），得到 \\mu_{\\theta}(x_t,t)=\\frac{1}{\\sqrt{\\alpha}_t}x_t-\\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}\\sqrt{\\alpha_t}}\\hat{\\epsilon}_\\theta(x_t,t) \\tag{10}从而也能得到$x_{t-1}$的分布。有研究发现预测噪声比预测$x_0$有更好的表现。 到此为止，就介绍完了DDPM的基本思想。它是一个加噪-去噪模型，加噪时的噪声$\\epsilon$作为标签来训练神经网络预测噪声的精度；去噪则在给定$x_T$的前提下，不断采样前一步时刻的图像，最终得到$x_0$。总结一下它的优缺点： 相比传统的GAN模型，采用了新的生成范式，实现了更高质量的图像生成； 模型需要的参数量很小，即U-Net的参数量加上$\\alpha_t$； 模型需要较多采样步骤才能得到较好的生成结果，需要在采样速度上优化； 模型是随机生成图像的，人类无法控制生成的方向。 参考文献[1] Understanding Diffusion Models: A Unified Perspective [2] Denoising Diffusion Probabilistic Models","categories":[],"tags":[{"name":"diffusion","slug":"diffusion","permalink":"https://overrideturing.github.io/tags/diffusion/"}]},{"title":"欢迎来到我的世界！！！Surprise, Yumou!","slug":"hello-world","date":"2024-11-03T16:17:57.145Z","updated":"2024-11-03T17:32:19.885Z","comments":true,"path":"2024/11/04/hello-world/","permalink":"https://overrideturing.github.io/2024/11/04/hello-world/","excerpt":"","text":"你一定想不到我是如何弄了这么个玩意儿的，这可是面试加分项！全靠下面这个：Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[],"tags":[{"name":"mathematic","slug":"mathematic","permalink":"https://overrideturing.github.io/tags/mathematic/"},{"name":"diffusion","slug":"diffusion","permalink":"https://overrideturing.github.io/tags/diffusion/"}]}